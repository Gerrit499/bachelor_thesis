{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9530853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: peft in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (0.15.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from peft) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from peft) (2.6.0+cu126)\n",
      "Requirement already satisfied: transformers in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from peft) (4.51.3)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from peft) (4.66.5)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from peft) (1.6.0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from peft) (0.30.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from huggingface_hub>=0.25.0->peft) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from huggingface_hub>=0.25.0->peft) (2024.6.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from huggingface_hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.13.0->peft) (70.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers->peft) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from transformers->peft) (0.21.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49c5857",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.6.0+cu126 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "LoRA applied modules and trainable parameters:\n",
      "trainable params: 3,145,728 || all params: 562,360,320 || trainable%: 0.5594\n",
      "\n",
      "LoRA layers applied:\n",
      "base_model.model.transformer.h.0.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.0.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.0.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.0.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.0.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.0.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.0.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.0.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.0.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.0.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.0.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.0.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.0.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.0.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.0.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.0.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.0.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.0.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.0.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.0.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.0.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.0.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.0.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.0.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.0.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.0.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.0.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.0.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.0.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.0.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.0.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.0.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.0.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.0.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.0.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.0.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.1.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.1.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.1.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.1.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.1.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.1.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.1.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.1.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.1.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.1.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.1.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.1.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.1.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.1.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.1.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.1.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.1.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.1.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.1.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.1.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.1.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.1.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.1.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.1.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.1.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.1.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.1.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.1.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.1.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.1.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.1.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.1.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.1.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.1.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.1.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.1.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.2.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.2.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.2.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.2.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.2.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.2.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.2.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.2.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.2.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.2.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.2.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.2.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.2.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.2.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.2.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.2.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.2.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.2.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.2.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.2.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.2.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.2.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.2.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.2.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.2.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.2.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.2.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.2.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.2.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.2.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.2.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.2.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.2.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.2.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.2.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.2.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.3.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.3.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.3.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.3.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.3.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.3.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.3.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.3.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.3.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.3.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.3.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.3.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.3.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.3.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.3.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.3.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.3.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.3.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.3.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.3.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.3.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.3.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.3.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.3.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.3.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.3.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.3.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.3.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.3.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.3.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.3.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.3.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.3.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.3.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.3.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.3.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.4.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.4.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.4.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.4.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.4.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.4.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.4.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.4.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.4.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.4.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.4.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.4.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.4.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.4.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.4.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.4.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.4.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.4.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.4.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.4.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.4.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.4.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.4.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.4.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.4.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.4.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.4.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.4.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.4.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.4.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.4.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.4.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.4.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.4.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.4.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.4.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.5.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.5.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.5.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.5.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.5.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.5.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.5.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.5.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.5.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.5.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.5.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.5.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.5.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.5.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.5.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.5.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.5.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.5.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.5.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.5.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.5.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.5.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.5.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.5.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.5.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.5.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.5.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.5.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.5.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.5.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.5.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.5.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.5.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.5.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.5.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.5.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.6.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.6.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.6.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.6.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.6.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.6.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.6.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.6.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.6.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.6.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.6.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.6.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.6.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.6.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.6.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.6.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.6.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.6.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.6.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.6.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.6.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.6.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.6.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.6.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.6.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.6.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.6.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.6.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.6.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.6.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.6.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.6.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.6.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.6.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.6.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.6.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.7.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.7.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.7.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.7.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.7.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.7.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.7.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.7.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.7.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.7.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.7.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.7.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.7.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.7.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.7.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.7.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.7.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.7.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.7.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.7.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.7.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.7.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.7.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.7.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.7.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.7.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.7.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.7.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.7.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.7.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.7.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.7.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.7.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.7.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.7.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.7.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.8.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.8.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.8.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.8.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.8.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.8.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.8.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.8.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.8.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.8.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.8.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.8.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.8.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.8.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.8.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.8.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.8.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.8.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.8.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.8.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.8.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.8.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.8.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.8.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.8.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.8.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.8.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.8.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.8.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.8.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.8.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.8.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.8.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.8.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.8.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.8.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.9.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.9.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.9.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.9.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.9.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.9.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.9.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.9.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.9.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.9.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.9.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.9.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.9.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.9.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.9.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.9.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.9.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.9.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.9.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.9.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.9.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.9.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.9.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.9.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.9.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.9.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.9.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.9.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.9.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.9.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.9.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.9.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.9.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.9.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.9.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.9.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.10.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.10.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.10.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.10.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.10.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.10.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.10.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.10.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.10.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.10.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.10.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.10.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.10.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.10.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.10.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.10.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.10.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.10.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.10.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.10.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.10.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.10.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.10.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.10.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.10.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.10.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.10.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.10.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.10.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.10.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.10.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.10.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.10.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.10.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.10.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.10.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.11.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.11.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.11.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.11.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.11.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.11.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.11.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.11.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.11.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.11.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.11.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.11.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.11.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.11.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.11.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.11.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.11.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.11.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.11.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.11.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.11.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.11.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.11.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.11.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.11.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.11.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.11.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.11.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.11.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.11.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.11.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.11.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.11.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.11.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.11.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.11.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.12.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.12.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.12.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.12.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.12.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.12.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.12.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.12.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.12.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.12.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.12.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.12.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.12.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.12.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.12.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.12.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.12.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.12.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.12.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.12.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.12.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.12.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.12.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.12.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.12.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.12.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.12.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.12.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.12.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.12.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.12.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.12.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.12.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.12.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.12.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.12.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.13.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.13.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.13.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.13.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.13.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.13.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.13.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.13.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.13.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.13.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.13.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.13.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.13.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.13.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.13.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.13.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.13.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.13.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.13.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.13.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.13.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.13.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.13.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.13.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.13.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.13.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.13.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.13.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.13.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.13.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.13.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.13.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.13.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.13.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.13.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.13.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.14.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.14.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.14.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.14.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.14.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.14.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.14.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.14.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.14.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.14.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.14.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.14.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.14.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.14.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.14.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.14.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.14.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.14.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.14.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.14.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.14.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.14.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.14.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.14.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.14.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.14.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.14.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.14.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.14.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.14.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.14.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.14.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.14.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.14.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.14.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.14.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.15.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.15.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.15.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.15.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.15.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.15.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.15.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.15.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.15.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.15.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.15.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.15.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.15.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.15.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.15.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.15.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.15.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.15.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.15.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.15.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.15.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.15.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.15.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.15.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.15.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.15.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.15.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.15.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.15.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.15.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.15.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.15.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.15.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.15.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.15.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.15.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.16.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.16.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.16.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.16.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.16.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.16.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.16.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.16.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.16.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.16.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.16.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.16.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.16.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.16.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.16.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.16.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.16.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.16.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.16.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.16.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.16.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.16.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.16.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.16.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.16.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.16.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.16.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.16.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.16.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.16.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.16.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.16.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.16.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.16.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.16.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.16.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.17.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.17.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.17.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.17.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.17.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.17.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.17.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.17.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.17.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.17.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.17.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.17.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.17.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.17.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.17.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.17.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.17.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.17.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.17.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.17.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.17.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.17.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.17.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.17.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.17.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.17.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.17.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.17.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.17.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.17.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.17.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.17.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.17.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.17.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.17.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.17.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.18.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.18.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.18.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.18.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.18.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.18.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.18.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.18.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.18.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.18.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.18.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.18.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.18.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.18.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.18.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.18.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.18.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.18.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.18.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.18.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.18.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.18.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.18.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.18.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.18.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.18.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.18.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.18.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.18.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.18.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.18.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.18.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.18.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.18.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.18.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.18.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.19.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.19.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.19.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.19.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.19.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.19.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.19.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.19.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.19.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.19.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.19.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.19.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.19.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.19.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.19.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.19.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.19.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.19.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.19.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.19.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.19.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.19.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.19.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.19.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.19.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.19.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.19.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.19.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.19.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.19.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.19.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.19.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.19.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.19.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.19.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.19.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.20.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.20.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.20.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.20.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.20.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.20.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.20.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.20.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.20.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.20.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.20.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.20.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.20.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.20.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.20.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.20.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.20.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.20.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.20.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.20.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.20.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.20.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.20.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.20.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.20.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.20.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.20.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.20.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.20.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.20.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.20.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.20.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.20.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.20.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.20.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.20.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.21.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.21.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.21.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.21.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.21.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.21.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.21.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.21.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.21.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.21.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.21.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.21.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.21.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.21.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.21.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.21.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.21.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.21.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.21.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.21.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.21.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.21.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.21.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.21.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.21.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.21.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.21.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.21.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.21.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.21.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.21.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.21.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.21.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.21.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.21.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.21.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.22.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.22.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.22.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.22.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.22.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.22.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.22.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.22.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.22.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.22.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.22.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.22.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.22.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.22.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.22.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.22.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.22.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.22.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.22.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.22.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.22.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.22.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.22.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.22.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.22.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.22.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.22.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.22.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.22.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.22.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.22.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.22.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.22.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.22.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.22.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.22.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.23.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.23.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.23.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.23.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.23.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.23.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.23.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.23.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.23.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.23.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.23.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.23.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.23.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.23.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.23.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.23.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.23.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.23.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.23.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.23.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.23.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.23.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.23.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.23.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.23.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.23.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.23.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.23.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.23.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.23.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.23.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.23.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.23.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.23.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.23.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.23.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31af032374d64c5f90978420dca447fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='226' max='226' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [226/226 05:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.963800</td>\n",
       "      <td>1.916888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.799600</td>\n",
       "      <td>1.816578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.861700</td>\n",
       "      <td>1.795121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.747500</td>\n",
       "      <td>1.774979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.816600</td>\n",
       "      <td>1.773060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.770100</td>\n",
       "      <td>1.765869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.800100</td>\n",
       "      <td>1.755674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.740600</td>\n",
       "      <td>1.750739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.822200</td>\n",
       "      <td>1.746488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.820900</td>\n",
       "      <td>1.740271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.762300</td>\n",
       "      <td>1.738521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n",
      "Training log updated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:\n",
      "  - 0: 1077989376 bytes required\n",
      "These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.\n",
      "Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:\n",
      "  - 0: 1078513664 bytes required\n",
      "These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.\n"
     ]
    }
   ],
   "source": [
    "# 1 Setup\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\".\"))\n",
    "\n",
    "import comet\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib as plt\n",
    "\n",
    "from engzho_avg_gpt_model import generate_translation, load_finetuned_model\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = load_finetuned_model()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./finetuned_lora_model_engzho_medium\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f7864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Data\n",
    "dataset = load_dataset(\"Muennighoff/flores200\", \"all\", revision=\"refs/pr/7\", trust_remote_code=True)\n",
    "dev_set = dataset[\"dev\"]\n",
    "lang_pairs = {\n",
    "    \"zho-eng\": dev_set.filter(lambda x: x[\"sentence_zho_Hans\"] and x[\"sentence_eng_Latn\"]),\n",
    "    \"eng-zho\": dev_set.filter(lambda x: x[\"sentence_eng_Latn\"] and x[\"sentence_zho_Hans\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e85a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gerri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\gerri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 3 BLEU and METEOR\n",
    "import sacrebleu\n",
    "\n",
    "def compute_bleu(predictions, references):\n",
    "    if isinstance(predictions, str):\n",
    "        predictions = [predictions]\n",
    "    if isinstance(references[0], str):\n",
    "        references = [[ref] for ref in references]\n",
    "\n",
    "    scores = []\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        score = sacrebleu.sentence_bleu(pred, ref).score\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "def compute_meteor(predictions, references):\n",
    "    if isinstance(predictions, str):\n",
    "        predictions = [predictions]\n",
    "    if isinstance(references, str):\n",
    "        references = [references]\n",
    "\n",
    "    scores = []\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        score = meteor_score([word_tokenize(ref)], word_tokenize(pred))\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5d02a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f0c94c21994c88bae3cf31fd11bcc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\gerri\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
      "Encoder model frozen.\n",
      "C:\\Users\\gerri\\AppData\\Roaming\\Python\\Python312\\site-packages\\pytorch_lightning\\core\\saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5617a4cdcf56488c8195046dd8e8cb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.3.5 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\gerri\\.cache\\huggingface\\hub\\models--Unbabel--wmt20-comet-qe-da\\snapshots\\2e7ffc84fb67d99cf92506611766463bb9230cfb\\checkpoints\\model.ckpt`\n",
      "Encoder model frozen.\n"
     ]
    }
   ],
   "source": [
    "# 4 COMET\n",
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "# Reference-based COMET\n",
    "comet_ref_model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "comet_ref_model = load_from_checkpoint(comet_ref_model_path)\n",
    "\n",
    "# Reference-free COMET\n",
    "cometkiwi_model_path = download_model(\"Unbabel/wmt22-cometkiwi-da\")\n",
    "cometkiwi_model = load_from_checkpoint(cometkiwi_model_path)\n",
    "\n",
    "\n",
    "# Safety check\n",
    "if \"comet_ref_model\" not in globals():\n",
    "    comet_ref_model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "    comet_ref_model = load_from_checkpoint(comet_ref_model_path)\n",
    "\n",
    "if \"cometkiwi_model\" not in globals():\n",
    "    cometkiwi_model_path = download_model(\"Unbabel/wmt22-cometkiwi-da\")\n",
    "    cometkiwi_model = load_from_checkpoint(cometkiwi_model_path)\n",
    "\n",
    "# Compute COMET scores\n",
    "def compute_comet_ref(srcs, mts, refs):\n",
    "    try:\n",
    "        data = [{\"src\": s, \"mt\": m, \"ref\": r} for s, m, r in zip(srcs, mts, refs)]\n",
    "        score = comet_ref_model.predict(data, gpus=1 if torch.cuda.is_available() else 0)\n",
    "        return score.scores\n",
    "    except Exception as e:\n",
    "        print(f\"[COMET-REF ERROR] {e}\")\n",
    "        return [float(\"nan\")] * len(srcs)\n",
    "\n",
    "def compute_cometkiwi(srcs, mts):\n",
    "    try:\n",
    "        data = [{\"src\": s, \"mt\": m} for s, m in zip(srcs, mts)]\n",
    "        score = cometkiwi_model.predict(data, gpus=1 if torch.cuda.is_available() else 0)\n",
    "        return score.scores\n",
    "    except Exception as e:\n",
    "        print(f\"[COMET-KIWI ERROR] {e}\")\n",
    "        return [float(\"nan\")] * len(srcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac68e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Results + getting translations\n",
    "def get_results_batched(examples, source_field, target_field, prompt_template, direction, results_list):\n",
    "    strategies = [\"greedy\"]\n",
    "\n",
    "    for strategy in strategies:\n",
    "        print(f\"\\n[Strategy: {strategy}]\")\n",
    "\n",
    "        prompts = []\n",
    "        sources = []\n",
    "        references = []\n",
    "\n",
    "        # Generate prompts and collect source/reference\n",
    "        for ex in examples:\n",
    "            source = ex[source_field]\n",
    "            reference = ex[target_field]\n",
    "            prompt = prompt_template.format(source=source)\n",
    "\n",
    "            prompts.append(prompt)\n",
    "            sources.append(source)\n",
    "            references.append(reference)\n",
    "\n",
    "        # Generate translations in batch + log probs + perplexities\n",
    "        translations = []\n",
    "        log_probs = []\n",
    "        perplexities = []\n",
    "        for prompt in tqdm(prompts, desc=f\"Translating ({strategy})\"):\n",
    "            try:\n",
    "                translation, log_prob, ppl = generate_translation(prompt, strategy)\n",
    "                translations.append(translation)\n",
    "                log_probs.append(log_prob)\n",
    "                perplexities.append(ppl)\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Strategy {strategy}: {e}\")\n",
    "                translations.append(\"\")\n",
    "                log_probs.append(float(\"nan\"))\n",
    "                perplexities.append(float(\"nan\"))\n",
    "\n",
    "        # Compute BLEU and METEOR in batch\n",
    "        bleu_scores = compute_bleu(translations, references)\n",
    "        meteor_scores = compute_meteor(translations, references)\n",
    "\n",
    "        # Compute COMET scores (aligned)\n",
    "        valid_indices = [i for i, t in enumerate(translations) if t.strip()]\n",
    "        comet_refs = [float(\"nan\")] * len(translations)\n",
    "        comet_wmt = [float(\"nan\")] * len(translations)\n",
    "\n",
    "        try:\n",
    "            valid_sources = [sources[i] for i in valid_indices]\n",
    "            valid_refs = [references[i] for i in valid_indices]\n",
    "            valid_trans = [translations[i] for i in valid_indices]\n",
    "\n",
    "            comet_ref_scores = compute_comet_ref(valid_sources, valid_trans, valid_refs)\n",
    "            comet_wmt_scores = compute_cometkiwi(valid_sources, valid_trans)\n",
    "\n",
    "            for j, idx in enumerate(valid_indices):\n",
    "                comet_refs[idx] = comet_ref_scores[j]\n",
    "                comet_wmt[idx] = comet_wmt_scores[j]\n",
    "        except Exception as e:\n",
    "            print(f\"[COMET ERROR] {e}\")\n",
    "\n",
    "        # Store results\n",
    "        for i in range(len(translations)):\n",
    "            results_list.append({\n",
    "                \"source\": sources[i],\n",
    "                \"reference\": references[i],\n",
    "                \"strategy\": strategy,\n",
    "                \"translation\": translations[i],\n",
    "                \"total_log_probs\": log_probs[i],\n",
    "                \"perplexity\": perplexities[i],\n",
    "                \"bleu\": bleu_scores[i],\n",
    "                \"meteor\": meteor_scores[i],\n",
    "                \"comet_ref\": comet_refs[i],\n",
    "                \"comet_wmt22\": comet_wmt[i]\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667337a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Strategy: greedy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating (greedy): 100%|██████████| 350/350 [27:18<00:00,  4.68s/it]\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Predicting DataLoader 0: 100%|██████████| 22/22 [00:04<00:00,  4.48it/s]\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Predicting DataLoader 0: 100%|██████████| 22/22 [00:03<00:00,  7.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# eng tp zho\n",
    "results_to_zho = []\n",
    "source_field = \"sentence_eng_Latn\"\n",
    "target_field = \"sentence_zho_Hans\"\n",
    "prompt_en2zh = \"\"\"Task: Translate the following English text to Chinese.\n",
    "\n",
    "English text: {source}\n",
    "\n",
    "Chinese translation:\"\"\".strip()\n",
    "\n",
    "get_results_batched(\n",
    "    examples=lang_pairs[\"eng-zho\"].select(range(350)),\n",
    "    source_field=source_field,\n",
    "    target_field=target_field,\n",
    "    prompt_template=prompt_en2zh,\n",
    "    direction=\"eng-zho\",\n",
    "    results_list=results_to_zho,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01d07cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destination results\n",
    "output_dir = \"csv_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf3f8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "                                              source  \\\n",
      "0  On Monday, scientists from the Stanford Univer...   \n",
      "1  Lead researchers say this may bring early dete...   \n",
      "2  The JAS 39C Gripen crashed onto a runway at ar...   \n",
      "3  The pilot was identified as Squadron Leader Di...   \n",
      "4  Local media reports an airport fire vehicle ro...   \n",
      "\n",
      "                                           reference strategy  \\\n",
      "0  周一，斯坦福大学医学院的科学家宣布，他们发明了一种可以将细胞按类型分类的新型诊断工具：一种可...   greedy   \n",
      "1  主要研究人员表示，这可以让低收入国家/地区的患者尽早发现癌症、肺结核、艾滋病和疟疾。在这些国...   greedy   \n",
      "2  当地时间上午 9:30 左右 (UTC 0230)，JAS 39C 鹰狮战斗机撞上跑道并发生...   greedy   \n",
      "3            涉事飞行员是空军中队长迪罗里·帕塔维 (Dilokrit Pattavee)。   greedy   \n",
      "4                           当地媒体报道，一辆机场消防车在响应火警时翻了车。   greedy   \n",
      "\n",
      "                                         translation  total_log_probs  \\\n",
      "0  星期一，斯坦福大学医学院的科学家宣布，他们发明了一种新的诊断工具，可以将细胞按类型分类：一种...       -36.498569   \n",
      "1  研究小组表示，这将有助于在低收入国家治疗癌症、结核病、艾滋病和疟疾，这些疾病在富裕国家生存率...       -19.657833   \n",
      "2  JAS 39C Gripen于当地时间上午约9:30（UTC0230）坠毁，并爆炸，关闭了该...       -13.575482   \n",
      "3         飞行员被确认为 Squadron Leader Dilokrit Pattavee。        -5.023162   \n",
      "4                               当地媒体报道，机场消防车在响应时翻了车。        -3.393498   \n",
      "\n",
      "   perplexity      bleu    meteor  comet_ref  comet_qe  \n",
      "0    2.139104  0.000000  0.000000   0.749809  0.055244  \n",
      "1    1.885369  0.000000  0.000000   0.828953  0.332089  \n",
      "2    1.528419  3.132600  0.053763   0.851735  0.538645  \n",
      "3    1.368818  8.745825  0.084746   0.752485  0.308467  \n",
      "4    1.253868  0.000000  0.000000   0.863002  0.173145  \n"
     ]
    }
   ],
   "source": [
    "# Store in CSV\n",
    "results_to_zho_df = pd.DataFrame(results_to_zho)\n",
    "results_to_zho_df.to_csv(os.path.join(output_dir, \"avg_dist_eng-zho_bloomz-560_flores200_results.csv\"), index=False)\n",
    "print(\"---------------\")\n",
    "print(results_to_zho_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
