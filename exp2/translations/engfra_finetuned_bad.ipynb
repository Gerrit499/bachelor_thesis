{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ece52181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: peft in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (0.15.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from peft) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from peft) (2.6.0+cu126)\n",
      "Requirement already satisfied: transformers in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from peft) (4.51.3)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from peft) (4.66.5)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from peft) (1.6.0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from peft) (0.30.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from huggingface_hub>=0.25.0->peft) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from huggingface_hub>=0.25.0->peft) (2024.6.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from huggingface_hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.13.0->peft) (70.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers->peft) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from transformers->peft) (0.21.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gerri\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49c5857",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.6.0+cu126 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "LoRA applied modules and trainable parameters:\n",
      "trainable params: 3,145,728 || all params: 562,360,320 || trainable%: 0.5594\n",
      "\n",
      "LoRA layers applied:\n",
      "base_model.model.transformer.h.0.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.0.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.0.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.0.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.0.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.0.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.0.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.0.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.0.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.0.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.0.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.0.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.0.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.0.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.0.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.0.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.0.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.0.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.0.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.0.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.0.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.0.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.0.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.0.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.0.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.0.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.0.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.0.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.0.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.0.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.0.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.0.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.0.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.0.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.0.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.0.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.1.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.1.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.1.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.1.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.1.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.1.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.1.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.1.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.1.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.1.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.1.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.1.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.1.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.1.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.1.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.1.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.1.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.1.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.1.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.1.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.1.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.1.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.1.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.1.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.1.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.1.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.1.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.1.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.1.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.1.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.1.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.1.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.1.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.1.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.1.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.1.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.2.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.2.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.2.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.2.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.2.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.2.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.2.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.2.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.2.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.2.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.2.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.2.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.2.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.2.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.2.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.2.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.2.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.2.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.2.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.2.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.2.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.2.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.2.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.2.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.2.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.2.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.2.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.2.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.2.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.2.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.2.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.2.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.2.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.2.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.2.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.2.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.3.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.3.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.3.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.3.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.3.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.3.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.3.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.3.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.3.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.3.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.3.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.3.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.3.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.3.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.3.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.3.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.3.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.3.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.3.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.3.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.3.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.3.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.3.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.3.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.3.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.3.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.3.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.3.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.3.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.3.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.3.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.3.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.3.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.3.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.3.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.3.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.4.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.4.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.4.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.4.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.4.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.4.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.4.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.4.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.4.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.4.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.4.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.4.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.4.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.4.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.4.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.4.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.4.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.4.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.4.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.4.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.4.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.4.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.4.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.4.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.4.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.4.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.4.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.4.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.4.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.4.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.4.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.4.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.4.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.4.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.4.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.4.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.5.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.5.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.5.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.5.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.5.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.5.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.5.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.5.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.5.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.5.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.5.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.5.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.5.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.5.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.5.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.5.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.5.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.5.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.5.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.5.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.5.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.5.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.5.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.5.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.5.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.5.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.5.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.5.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.5.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.5.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.5.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.5.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.5.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.5.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.5.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.5.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.6.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.6.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.6.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.6.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.6.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.6.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.6.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.6.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.6.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.6.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.6.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.6.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.6.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.6.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.6.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.6.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.6.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.6.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.6.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.6.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.6.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.6.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.6.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.6.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.6.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.6.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.6.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.6.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.6.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.6.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.6.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.6.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.6.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.6.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.6.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.6.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.7.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.7.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.7.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.7.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.7.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.7.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.7.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.7.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.7.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.7.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.7.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.7.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.7.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.7.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.7.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.7.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.7.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.7.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.7.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.7.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.7.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.7.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.7.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.7.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.7.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.7.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.7.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.7.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.7.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.7.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.7.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.7.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.7.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.7.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.7.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.7.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.8.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.8.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.8.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.8.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.8.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.8.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.8.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.8.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.8.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.8.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.8.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.8.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.8.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.8.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.8.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.8.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.8.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.8.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.8.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.8.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.8.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.8.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.8.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.8.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.8.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.8.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.8.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.8.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.8.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.8.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.8.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.8.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.8.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.8.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.8.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.8.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.9.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.9.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.9.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.9.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.9.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.9.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.9.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.9.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.9.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.9.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.9.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.9.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.9.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.9.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.9.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.9.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.9.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.9.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.9.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.9.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.9.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.9.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.9.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.9.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.9.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.9.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.9.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.9.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.9.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.9.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.9.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.9.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.9.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.9.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.9.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.9.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.10.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.10.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.10.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.10.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.10.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.10.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.10.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.10.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.10.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.10.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.10.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.10.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.10.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.10.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.10.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.10.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.10.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.10.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.10.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.10.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.10.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.10.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.10.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.10.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.10.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.10.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.10.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.10.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.10.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.10.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.10.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.10.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.10.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.10.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.10.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.10.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.11.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.11.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.11.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.11.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.11.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.11.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.11.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.11.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.11.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.11.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.11.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.11.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.11.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.11.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.11.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.11.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.11.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.11.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.11.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.11.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.11.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.11.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.11.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.11.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.11.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.11.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.11.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.11.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.11.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.11.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.11.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.11.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.11.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.11.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.11.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.11.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.12.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.12.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.12.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.12.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.12.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.12.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.12.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.12.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.12.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.12.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.12.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.12.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.12.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.12.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.12.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.12.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.12.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.12.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.12.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.12.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.12.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.12.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.12.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.12.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.12.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.12.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.12.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.12.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.12.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.12.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.12.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.12.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.12.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.12.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.12.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.12.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.13.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.13.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.13.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.13.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.13.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.13.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.13.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.13.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.13.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.13.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.13.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.13.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.13.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.13.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.13.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.13.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.13.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.13.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.13.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.13.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.13.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.13.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.13.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.13.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.13.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.13.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.13.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.13.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.13.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.13.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.13.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.13.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.13.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.13.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.13.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.13.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.14.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.14.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.14.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.14.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.14.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.14.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.14.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.14.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.14.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.14.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.14.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.14.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.14.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.14.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.14.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.14.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.14.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.14.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.14.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.14.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.14.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.14.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.14.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.14.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.14.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.14.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.14.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.14.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.14.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.14.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.14.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.14.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.14.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.14.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.14.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.14.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.15.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.15.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.15.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.15.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.15.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.15.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.15.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.15.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.15.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.15.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.15.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.15.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.15.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.15.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.15.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.15.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.15.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.15.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.15.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.15.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.15.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.15.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.15.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.15.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.15.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.15.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.15.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.15.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.15.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.15.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.15.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.15.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.15.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.15.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.15.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.15.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.16.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.16.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.16.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.16.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.16.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.16.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.16.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.16.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.16.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.16.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.16.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.16.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.16.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.16.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.16.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.16.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.16.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.16.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.16.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.16.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.16.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.16.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.16.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.16.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.16.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.16.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.16.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.16.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.16.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.16.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.16.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.16.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.16.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.16.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.16.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.16.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.17.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.17.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.17.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.17.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.17.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.17.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.17.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.17.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.17.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.17.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.17.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.17.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.17.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.17.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.17.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.17.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.17.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.17.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.17.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.17.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.17.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.17.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.17.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.17.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.17.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.17.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.17.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.17.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.17.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.17.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.17.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.17.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.17.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.17.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.17.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.17.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.18.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.18.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.18.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.18.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.18.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.18.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.18.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.18.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.18.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.18.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.18.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.18.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.18.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.18.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.18.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.18.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.18.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.18.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.18.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.18.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.18.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.18.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.18.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.18.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.18.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.18.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.18.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.18.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.18.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.18.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.18.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.18.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.18.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.18.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.18.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.18.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.19.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.19.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.19.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.19.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.19.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.19.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.19.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.19.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.19.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.19.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.19.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.19.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.19.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.19.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.19.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.19.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.19.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.19.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.19.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.19.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.19.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.19.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.19.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.19.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.19.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.19.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.19.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.19.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.19.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.19.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.19.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.19.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.19.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.19.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.19.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.19.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.20.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.20.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.20.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.20.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.20.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.20.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.20.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.20.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.20.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.20.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.20.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.20.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.20.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.20.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.20.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.20.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.20.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.20.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.20.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.20.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.20.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.20.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.20.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.20.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.20.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.20.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.20.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.20.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.20.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.20.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.20.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.20.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.20.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.20.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.20.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.20.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.21.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.21.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.21.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.21.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.21.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.21.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.21.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.21.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.21.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.21.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.21.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.21.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.21.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.21.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.21.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.21.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.21.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.21.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.21.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.21.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.21.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.21.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.21.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.21.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.21.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.21.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.21.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.21.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.21.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.21.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.21.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.21.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.21.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.21.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.21.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.21.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.22.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.22.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.22.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.22.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.22.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.22.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.22.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.22.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.22.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.22.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.22.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.22.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.22.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.22.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.22.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.22.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.22.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.22.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.22.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.22.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.22.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.22.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.22.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.22.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.22.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.22.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.22.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.22.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.22.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.22.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.22.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.22.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.22.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.22.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.22.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.22.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.23.self_attention.query_key_value.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.23.self_attention.query_key_value.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.23.self_attention.query_key_value.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.23.self_attention.query_key_value.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.23.self_attention.query_key_value.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.23.self_attention.query_key_value.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.23.self_attention.query_key_value.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.23.self_attention.query_key_value.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.23.self_attention.query_key_value.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.23.self_attention.dense.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.23.self_attention.dense.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.23.self_attention.dense.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.23.self_attention.dense.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.23.self_attention.dense.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.23.self_attention.dense.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.23.self_attention.dense.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.23.self_attention.dense.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.23.self_attention.dense.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.23.mlp.dense_h_to_4h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.23.mlp.dense_h_to_4h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.23.mlp.dense_h_to_4h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.23.mlp.dense_h_to_4h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.23.mlp.dense_h_to_4h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.23.mlp.dense_h_to_4h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.23.mlp.dense_h_to_4h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.23.mlp.dense_h_to_4h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.23.mlp.dense_h_to_4h.lora_magnitude_vector -> ModuleDict\n",
      "base_model.model.transformer.h.23.mlp.dense_4h_to_h.lora_dropout -> ModuleDict\n",
      "base_model.model.transformer.h.23.mlp.dense_4h_to_h.lora_dropout.default -> Dropout\n",
      "base_model.model.transformer.h.23.mlp.dense_4h_to_h.lora_A -> ModuleDict\n",
      "base_model.model.transformer.h.23.mlp.dense_4h_to_h.lora_A.default -> Linear\n",
      "base_model.model.transformer.h.23.mlp.dense_4h_to_h.lora_B -> ModuleDict\n",
      "base_model.model.transformer.h.23.mlp.dense_4h_to_h.lora_B.default -> Linear\n",
      "base_model.model.transformer.h.23.mlp.dense_4h_to_h.lora_embedding_A -> ParameterDict\n",
      "base_model.model.transformer.h.23.mlp.dense_4h_to_h.lora_embedding_B -> ParameterDict\n",
      "base_model.model.transformer.h.23.mlp.dense_4h_to_h.lora_magnitude_vector -> ModuleDict\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b799b33f402412c923cee6402586ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='226' max='226' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [226/226 05:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.599700</td>\n",
       "      <td>1.558584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.553900</td>\n",
       "      <td>1.460701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.368900</td>\n",
       "      <td>1.453080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.508600</td>\n",
       "      <td>1.434866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.397000</td>\n",
       "      <td>1.430604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.359500</td>\n",
       "      <td>1.421253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.444400</td>\n",
       "      <td>1.416487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.476700</td>\n",
       "      <td>1.413476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.454900</td>\n",
       "      <td>1.409141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.335000</td>\n",
       "      <td>1.407048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.337100</td>\n",
       "      <td>1.405281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n",
      "Training log updated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:\n",
      "  - 0: 1077989376 bytes required\n",
      "These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.\n",
      "Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:\n",
      "  - 0: 1078513664 bytes required\n",
      "These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.\n"
     ]
    }
   ],
   "source": [
    "# 1 Setup\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\".\"))\n",
    "\n",
    "import comet\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib as plt\n",
    "\n",
    "from engfra_bad_gpt_model import generate_translation, load_finetuned_model\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = load_finetuned_model()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./finetuned_lora_model_engfra_low\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f7864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Data\n",
    "dataset = load_dataset(\"Muennighoff/flores200\", \"all\", revision=\"refs/pr/7\", trust_remote_code=True)\n",
    "dev_set = dataset[\"dev\"]\n",
    "lang_pairs = {\n",
    "    \"fra-eng\": dev_set.filter(lambda x: x[\"sentence_fra_Latn\"] and x[\"sentence_eng_Latn\"]),\n",
    "    \"eng-fra\": dev_set.filter(lambda x: x[\"sentence_eng_Latn\"] and x[\"sentence_fra_Latn\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e85a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gerri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\gerri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 3 BLEU and METEOR\n",
    "import sacrebleu\n",
    "\n",
    "def compute_bleu(predictions, references):\n",
    "    if isinstance(predictions, str):\n",
    "        predictions = [predictions]\n",
    "    if isinstance(references[0], str):\n",
    "        references = [[ref] for ref in references]\n",
    "\n",
    "    scores = []\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        score = sacrebleu.sentence_bleu(pred, ref).score\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "def compute_meteor(predictions, references):\n",
    "    if isinstance(predictions, str):\n",
    "        predictions = [predictions]\n",
    "    if isinstance(references, str):\n",
    "        references = [references]\n",
    "\n",
    "    scores = []\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        score = meteor_score([word_tokenize(ref)], word_tokenize(pred))\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5d02a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5238a4bc11984a35be26fb9109ed86fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\gerri\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
      "Encoder model frozen.\n",
      "C:\\Users\\gerri\\AppData\\Roaming\\Python\\Python312\\site-packages\\pytorch_lightning\\core\\saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167ea8338e5144528eb96a93620c3537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.3.5 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\gerri\\.cache\\huggingface\\hub\\models--Unbabel--wmt20-comet-qe-da\\snapshots\\2e7ffc84fb67d99cf92506611766463bb9230cfb\\checkpoints\\model.ckpt`\n",
      "Encoder model frozen.\n"
     ]
    }
   ],
   "source": [
    "# 4 COMET\n",
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "# Reference-based COMET\n",
    "comet_ref_model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "comet_ref_model = load_from_checkpoint(comet_ref_model_path)\n",
    "\n",
    "# Reference-free COMET\n",
    "cometkiwi_model_path = download_model(\"Unbabel/wmt22-cometkiwi-da\")\n",
    "cometkiwi_model = load_from_checkpoint(cometkiwi_model_path)\n",
    "\n",
    "\n",
    "# Safety check\n",
    "if \"comet_ref_model\" not in globals():\n",
    "    comet_ref_model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "    comet_ref_model = load_from_checkpoint(comet_ref_model_path)\n",
    "\n",
    "if \"cometkiwi_model\" not in globals():\n",
    "    cometkiwi_model_path = download_model(\"Unbabel/wmt22-cometkiwi-da\")\n",
    "    cometkiwi_model = load_from_checkpoint(cometkiwi_model_path)\n",
    "\n",
    "# Compute COMET scores\n",
    "def compute_comet_ref(srcs, mts, refs):\n",
    "    try:\n",
    "        data = [{\"src\": s, \"mt\": m, \"ref\": r} for s, m, r in zip(srcs, mts, refs)]\n",
    "        score = comet_ref_model.predict(data, gpus=1 if torch.cuda.is_available() else 0)\n",
    "        return score.scores\n",
    "    except Exception as e:\n",
    "        print(f\"[COMET-REF ERROR] {e}\")\n",
    "        return [float(\"nan\")] * len(srcs)\n",
    "\n",
    "def compute_cometkiwi(srcs, mts):\n",
    "    try:\n",
    "        data = [{\"src\": s, \"mt\": m} for s, m in zip(srcs, mts)]\n",
    "        score = cometkiwi_model.predict(data, gpus=1 if torch.cuda.is_available() else 0)\n",
    "        return score.scores\n",
    "    except Exception as e:\n",
    "        print(f\"[COMET-KIWI ERROR] {e}\")\n",
    "        return [float(\"nan\")] * len(srcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac68e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Results + getting translations\n",
    "def get_results_batched(examples, source_field, target_field, prompt_template, direction, results_list):\n",
    "    strategies = [\"greedy\"]\n",
    "\n",
    "    for strategy in strategies:\n",
    "        print(f\"\\n[Strategy: {strategy}]\")\n",
    "\n",
    "        prompts = []\n",
    "        sources = []\n",
    "        references = []\n",
    "\n",
    "        # Generate prompts and collect source/reference\n",
    "        for ex in examples:\n",
    "            source = ex[source_field]\n",
    "            reference = ex[target_field]\n",
    "            prompt = prompt_template.format(source=source)\n",
    "\n",
    "            prompts.append(prompt)\n",
    "            sources.append(source)\n",
    "            references.append(reference)\n",
    "\n",
    "        # Generate translations in batch + log probs + perplexities\n",
    "        translations = []\n",
    "        log_probs = []\n",
    "        perplexities = []\n",
    "        for prompt in tqdm(prompts, desc=f\"Translating ({strategy})\"):\n",
    "            try:\n",
    "                translation, log_prob, ppl = generate_translation(prompt, strategy)\n",
    "                translations.append(translation)\n",
    "                log_probs.append(log_prob)\n",
    "                perplexities.append(ppl)\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Strategy {strategy}: {e}\")\n",
    "                translations.append(\"\")\n",
    "                log_probs.append(float(\"nan\"))\n",
    "                perplexities.append(float(\"nan\"))\n",
    "\n",
    "        # Compute BLEU and METEOR in batch\n",
    "        bleu_scores = compute_bleu(translations, references)\n",
    "        meteor_scores = compute_meteor(translations, references)\n",
    "\n",
    "        # Compute COMET scores (aligned)\n",
    "        valid_indices = [i for i, t in enumerate(translations) if t.strip()]\n",
    "        comet_refs = [float(\"nan\")] * len(translations)\n",
    "        comet_wmt = [float(\"nan\")] * len(translations)\n",
    "\n",
    "        try:\n",
    "            valid_sources = [sources[i] for i in valid_indices]\n",
    "            valid_refs = [references[i] for i in valid_indices]\n",
    "            valid_trans = [translations[i] for i in valid_indices]\n",
    "\n",
    "            comet_ref_scores = compute_comet_ref(valid_sources, valid_trans, valid_refs)\n",
    "            comet_wmt_scores = compute_cometkiwi(valid_sources, valid_trans)\n",
    "\n",
    "            for j, idx in enumerate(valid_indices):\n",
    "                comet_refs[idx] = comet_ref_scores[j]\n",
    "                comet_wmt[idx] = comet_wmt_scores[j]\n",
    "        except Exception as e:\n",
    "            print(f\"[COMET ERROR] {e}\")\n",
    "\n",
    "        # Store results\n",
    "        for i in range(len(translations)):\n",
    "            results_list.append({\n",
    "                \"source\": sources[i],\n",
    "                \"reference\": references[i],\n",
    "                \"strategy\": strategy,\n",
    "                \"translation\": translations[i],\n",
    "                \"total_log_probs\": log_probs[i],\n",
    "                \"perplexity\": perplexities[i],\n",
    "                \"bleu\": bleu_scores[i],\n",
    "                \"meteor\": meteor_scores[i],\n",
    "                \"comet_ref\": comet_refs[i],\n",
    "                \"comet_wmt22\": comet_wmt[i]\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6937412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destination results\n",
    "output_dir = \"csv_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667337a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Strategy: greedy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating (greedy): 100%|| 250/250 [23:58<00:00,  5.76s/it]\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Predicting DataLoader 0: 100%|| 16/16 [00:03<00:00,  4.37it/s]\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Predicting DataLoader 0: 100%|| 16/16 [00:02<00:00,  7.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# eng to fra\n",
    "results_to_fra = []\n",
    "source_field = \"sentence_eng_Latn\"\n",
    "target_field = \"sentence_fra_Latn\"\n",
    "prompt_en2zh = \"\"\"Task: Translate the following English text to French.\n",
    "\n",
    "English text: {source}\n",
    "\n",
    "French translation:\"\"\".strip()\n",
    "\n",
    "get_results_batched(\n",
    "    examples=lang_pairs[\"eng-fra\"].select(range(250)),\n",
    "    source_field=source_field,\n",
    "    target_field=target_field,\n",
    "    prompt_template=prompt_en2zh,\n",
    "    direction=\"eng-fra\",\n",
    "    results_list=results_to_fra,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf3f8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "                                              source  \\\n",
      "0  On Monday, scientists from the Stanford Univer...   \n",
      "1  Lead researchers say this may bring early dete...   \n",
      "2  The JAS 39C Gripen crashed onto a runway at ar...   \n",
      "3  The pilot was identified as Squadron Leader Di...   \n",
      "4  Local media reports an airport fire vehicle ro...   \n",
      "\n",
      "                                           reference strategy  \\\n",
      "0  Des scientifiques de lcole de mdecine de l...   greedy   \n",
      "1  Selon les chercheurs principaux, cela pourrait...   greedy   \n",
      "2  Le JAS39C Gripen sest cras sur une piste a...   greedy   \n",
      "3  Le pilote a t identifi comme tant le chef ...   greedy   \n",
      "4  La presse locale a rapport qu'un vhicule de ...   greedy   \n",
      "\n",
      "                                         translation  total_log_probs  \\\n",
      "0  Le lundi, des scientifiques de l'cole de mde...       -25.285719   \n",
      "1  Les chercheurs disent que cela pourrait apport...       -19.560266   \n",
      "2  Le JAS 39C Gripen a heurt un runway  environ...       -14.110737   \n",
      "3  Le pilote a t identifi comme le chef de la ...        -3.029746   \n",
      "4  Les mdias locaux rapportent qu'un vhicule de...       -10.837398   \n",
      "\n",
      "   perplexity       bleu    meteor  comet_ref  comet_qe  \n",
      "0    1.524137  18.914608  0.449353   0.822728  0.203773  \n",
      "1    1.370928  23.351856  0.514874   0.854602  0.429749  \n",
      "2    1.423002  28.716134  0.513699   0.863337  0.323816  \n",
      "3    1.163564  48.415247  0.730767   0.859133  0.383753  \n",
      "4    1.435117  22.451662  0.324656   0.497615 -0.084414  \n"
     ]
    }
   ],
   "source": [
    "# Store in CSV\n",
    "results_to_fra_df = pd.DataFrame(results_to_fra)\n",
    "results_to_fra_df.to_csv(os.path.join(output_dir, \"bad_dist_eng-fra_bloomz-560_flores200_results.csv\"), index=False)\n",
    "print(\"---------------\")\n",
    "print(results_to_fra_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
